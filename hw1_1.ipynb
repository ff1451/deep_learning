{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e768f3",
   "metadata": {},
   "source": [
    "# a_tensor_initialization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2270713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2787eac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "[추가 코드] t1 values: tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor class\n",
    "\n",
    "# [핵심] Tensor 생성: 리스트 [1,2,3]을 float32 타입의 Tensor로 생성\n",
    "# 기본적으로 torch.Tensor()는 dtype=float32로 초기화됨\n",
    "# device='cpu'를 지정했으므로 CPU 메모리에 생성됨\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "\n",
    "# [기능] dtype 출력: 텐서 내부 데이터 타입 확인\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "\n",
    "# [기능] device 출력: 해당 텐서가 저장된 장치 (CPU/GPU) 확인\n",
    "print(t1.device)  # >>> cpu\n",
    "\n",
    "# [기능] requires_grad 출력: 자동 미분(autograd) 추적 여부\n",
    "# 기본값은 False -> 학습 파라미터가 아님\n",
    "print(t1.requires_grad)  # >>> False\n",
    "\n",
    "# [기능] size()와 shape: 텐서 차원과 크기 확인\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])\n",
    "\n",
    "# [추가 코드] 텐서 값 직접 확인\n",
    "print(\"[추가 코드] t1 values:\", t1)  # >>> tensor([1., 2., 3.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49542ae",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "\n",
    "- Tensor 기본 생성 방식: torch.Tensor()는 항상 float32 타입이 기본, 정수 리스트를 넣어도 부동소수(float) 텐서가 됨 \n",
    "  - 만약 정수형이 필요하다면 torch.IntTensor, torch.tensor([...], dtype=torch.int32) 등을 명시\n",
    "\n",
    "- device : CPU/GPU 전환 시 .to(device) 또는 .cuda()를 활용\n",
    "  - 모델과 입력이 같은 device에 있어야 연산 오류가 발생하지 않음\n",
    "\n",
    "- requires_grad: 학습 시 파라미터로 쓰이는 텐서는 requires_grad=True로 설정해야 함. 그렇지 않으면 역전파 시 그래디언트가 계산되지 않음.\n",
    "\n",
    "- shape/size 동일성: tensor.shape와 tensor.size()는 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a11d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[추가 코드] GPU(cuda) 사용가능 여부 체크 :  False\n",
      "[추가 코드] t1_cpu device: cpu\n"
     ]
    }
   ],
   "source": [
    "# if you have gpu device\n",
    "\n",
    "# [추가 코드] GPU 사용 가능 여부 체크\n",
    "print(\"[추가 코드] GPU(cuda) 사용가능 여부 체크 : \",torch.cuda.is_available()) # >>> False : 노트북에서 GPU 미지원 중\n",
    "# [기능] GPU 장치로 Tensor를 옮기기\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "\n",
    "# [기능] CPU로 Tensor를 옮기기\n",
    "t1_cpu = t1.cpu()\n",
    "\n",
    "# [추가 코드] 실제 device 확인\n",
    "print(\"[추가 코드] t1_cpu device:\", t1_cpu.device)  # >>> cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82f362",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "\n",
    "- 장치 이동 (.to(), .cuda(), .cpu())\n",
    "  PyTorch는 GPU(CUDA)와 CPU 간 데이터 이동을 명확하게 해줘야 한다.\n",
    "\n",
    "  - .to(torch.device(\"cuda\")): 가장 범용적인 방법. device 객체를 직접 지정.\n",
    "\n",
    "  - .cuda(): GPU로 이동하는 shorthand.\n",
    "\n",
    "  - .cpu(): CPU로 이동.\n",
    "\n",
    "- GPU 학습 시 모델과 데이터 모두 동일한 device에 있어야 연산 가능하다.\n",
    "\n",
    "- GPU가 없는 환경에서 .cuda()를 호출하면 에러 (AssertionError: Torch not compiled with CUDA enabled)가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da9f4384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2cbd4",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "\n",
    "- 위 t1의 코드와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    }
   ],
   "source": [
    "# [기능] 스칼라 텐서 : 0차원 텐서\n",
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "# [기능] 벡터 텐서 : 1차원 텐서, 원소 1개\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "# [기능] 스칼라 텐서 : 1차원 텐서, 원소 5개\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "# [기능] 행렬 텐서 : 2차원 텐서, 5행 1열\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "# [기능] 행렬 텐서 : 2차원 텐서, 3행 2열\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "# [기능] 3차원 텐서, (3, 2, 1)\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "# [기능] 4차원 텐서, (3, 1, 2, 1)\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "# [기능] 4차원 텐서, (3, 1, 2, 3)\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "# [기능] 5차원 텐서, (3, 1, 2, 3, 1)\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "# [기능] 2차원 행렬, 4행 5열\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "# [기능] 3차원 행렬, (4, 1, 5)\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4917c4d",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- 스칼라 → 벡터 → 행렬 → 고차원 텐서: PyTorch 텐서는 rank(차원 수)에 따라 표현이 달라짐\n",
    "  - 스칼라: []\n",
    "  - 벡터: [N]\n",
    "  - 행렬: [M, N]\n",
    "  - 그 이상은 배치(batch)나 채널(channel) 등을 포함하는 고차원 텐서.\n",
    "\n",
    "- 차원(ndim)와 shape의 관계\n",
    "  - ndim(rank)는 몇 차원인지 알려주고, shape는 각 차원의 크기를 튜플 형태로 나타냄\n",
    "\n",
    "- 같은 원소 개수를 가지고 있어도 차원을 늘려서 다른 텐서를 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04bb2470",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# [핵심] 아래 데이터는 마지막 축의 길이가 서로 다름(3 vs 2)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 첫 번째 inner list: [1,2,3] : 길이 3\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 두 번째 inner list: [4,5]   : 길이 2\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 따라서 길이가 3과 2로 다름\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# PyTorch 텐서는 '직사각형' 모양(모든 차원 길이 동일)을 요구하므로 ValueError 발생\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m a11 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "# [핵심] 아래 데이터는 마지막 축의 길이가 서로 다름(3 vs 2)\n",
    "# 첫 번째 inner list: [1,2,3] : 길이 3\n",
    "# 두 번째 inner list: [4,5]   : 길이 2\n",
    "# 따라서 길이가 3과 2로 다름\n",
    "# PyTorch 텐서는 '직사각형' 모양(모든 차원 길이 동일)을 요구하므로 ValueError 발생\n",
    "a11 = torch.tensor([  # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26286f",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- PyTorch 텐서의 전제: 텐서는 모든 축에서 길이가 동일한 직사각형 배열이어야 함\n",
    "  - 가변 길이(불규칙) 목록은 그대로는 텐서로 변환할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7374f",
   "metadata": {},
   "source": [
    "# b_tensor_initialization_copy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99f0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98058fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "# [핵심] torch.Tensor()\n",
    "# 항상 float32 dtype\n",
    "# 입력 데이터를 '복사'하여 텐서 생성\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "# [핵심] torch.tensor()\n",
    "# int 입력 시 int 그대로 사용\n",
    "# 데이터를 '복사'하여 새로운 텐서 생성\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "# [핵심] torch.as_tensor()\n",
    "# 원본 데이터를 '공유'하려고 시도 (가능하면 복사하지 않음)\n",
    "\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ca927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "# 위 코드와 다른 점\n",
    "# - 위 코드는 기본 파이썬 리스트를 데이터로 사용\n",
    "# - 이 코드는 numpy의 array를 데이터로 사용\n",
    "\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f6285d",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- torch.Tensor()\n",
    "  - 오래된 생성자 스타일\n",
    "  - 항상 dtype=torch.float32로 강제 변환\n",
    "  - 데이터를 복사하므로 원본 리스트 변경과 무관\n",
    "\n",
    "- torch.tensor()\n",
    "  - 더 현대적인 API\n",
    "  - 입력 데이터의 타입 사용\n",
    "  - 데이터를 복사하므로 원본 리스트 변경과 무관\n",
    "\n",
    "- torch.as_tensor()\n",
    "  - 가능하면 데이터 공유(no-copy).\n",
    "  - numpy array를 넘기면 원본 메모리를 그대로 참조하여 성능상 유리\n",
    "\n",
    "- 데이터 공유 장단점\n",
    "  - 장점: 불필요한 복사 없이 속도↑, 메모리↓. 특히 대규모 데이터셋 로딩 시 유리.\n",
    "  - 단점: 원본 변경이 학습 중간에 반영되어 의도치 않은 버그 발생 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "337f9e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "[추가 코드] after fill: tensor([7., 7., 7., 7.])\n",
      "[추가 코드] after fill_like: tensor([-1., -1., -1., -1.])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [기능] torch.ones\n",
    "# 모든 원소가 1인 텐서 생성\n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "\n",
    "# [기능] torch.ones_like\n",
    "# 주어진 텐서와 동일한 shape, dtype, device로 1이 채워진 텐서를 생성\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "# [기능] torch.zeros(size=(6,))\n",
    "# 모든 원소가 0인 텐서 생성\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "\n",
    "# [기능] torch.zeros_like\n",
    "# 주어진 텐서와 동일한 shape, dtype, device로 0이 채워진 텐서 생성\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "\n",
    "# [기능] torch.empty(size=(4,))\n",
    "# 메모리만 할당하고 초기화하지 않음\n",
    "# 따라서 출력 값은 '쓰레기값'(random garbage)\n",
    "# 연산 효율을 위해 필요할 때만 초기화하는 방식\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
    "\n",
    "# [기능] torch.empty_like\n",
    "# 입력 텐서의 메타데이터를 복사하여 초기화 없는 새 텐서 생성\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "# [추가 코드] fill_ 메소드 사용: inplace로 값 채우기\n",
    "t3.fill_(7)       # t3의 모든 원소를 7로 덮어씀\n",
    "t3_like.fill_(-1) # t3_like의 모든 원소를 -1로 덮어씀\n",
    "print(\"[추가 코드] after fill:\", t3)\n",
    "print(\"[추가 코드] after fill_like:\", t3_like)\n",
    "\n",
    "\n",
    "# [핵심] torch.eye(n=3)\n",
    "# 대각행렬(identity matrix) 생성\n",
    "# shape=(3,3), 대각선=1, 나머지=0\n",
    "t4 = torch.eye(n=3)\n",
    "print(t4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376f632",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- ones / zeros / empty\n",
    "  - ones: 모든 원소=1 → weight 초기화 시 특정 경우 유용.\n",
    "  - zeros: 모든 원소=0 → bias 초기화 등에 흔히 사용.\n",
    "  - empty: 메모리만 확보. 빠르지만 값 보장 없음 → 바로 덮어쓸 때 사용.\n",
    "\n",
    "- like 계열 (*_like)\n",
    "  - 기존 텐서와 동일한 shape, dtype, device로 새로운 텐서를 생성.\n",
    "  - 모델 파라미터와 동일 구조의 텐서를 만들 때 편리하다.\n",
    "  - 예: grad = torch.zeros_like(weight)\n",
    "\n",
    "- torch.eye\n",
    "  - 단위행렬(Identity Matrix)은 선형대수 핵심 개념.\n",
    "  - 신경망 구조에서 종종 identity transform이나 마스크 행렬로 활용됨.\n",
    "\n",
    "- fill_(): 텐서의 모든 원소를 동일한 값으로 in-place 채운다.\n",
    "  - 이름 끝의 _는 in-place 연산임을 의미. (메모리 새로 할당 X)\n",
    "  - t.fill_(x)는 성능적으로도 효율적이고 메모리 재사용 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "567e0a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 10]])\n",
      "tensor([[0.6146, 0.5999, 0.5013]])\n",
      "tensor([[-0.5975, -0.0649,  0.3680]])\n",
      "tensor([[ 9.7003,  8.7733],\n",
      "        [10.5015,  9.9954],\n",
      "        [10.2038,  9.7056]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [기능] randint: 정수 난수 생성\n",
    "# low ≤ 값 < high 범위에서 균일분포 난수 생성\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "# [기능] rand: [0,1) 범위의 균일분포 난수 생성 (float)\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "# [기능] randn: 표준정규분포(평균=0, 표준편차=1)에서 난수 생성 (float)\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "# [기능] normal: 지정된 평균(mean)과 표준편차(std)로 정규분포 난수 생성\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)\n",
    "\n",
    "# [기능] linspace: start~end 구간을 steps개로 균등 분할\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3) #0.0에서 ~ 5.0을 3개로 균등 분할하므로 [0.0, 2.5, 5.0]\n",
    "print(t5)\n",
    "\n",
    "# [핵심] arange: range처럼 정수 증가 시퀀스 생성\n",
    "t6 = torch.arange(5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4164f",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- 난수 함수\n",
    "  - randint: 정수 균일분포\n",
    "  - rand: [0,1) 구간 실수 균일분포\n",
    "  - randn: 표준정규분포\n",
    "  - normal: 지정된 평균/표준편차 정규분포\n",
    "  - linspace: 구간 균등 분할 (주로 그래프 그릴 때 유용)\n",
    "  - arange: 파이썬 range()와 유사, step도 지정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "893bf393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "[추가 코드] random1 == random3: True\n",
      "[추가 코드] random2 == random4: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심] 시드 고정 → 다시 해도 동일한 랜덤 값 나올 수 있도록 함\n",
    "torch.manual_seed(1729)\n",
    "\n",
    "# 동일 시드에서 생성된 난수는 항상 동일한 결과가 나옴\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 시드 다시 고정 후 같은 난수 순서 재현\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)\n",
    "\n",
    "print()\n",
    "\n",
    "# [추가 코드] 두 난수 결과 비교\n",
    "print(\"[추가 코드] random1 == random3:\", torch.equal(random1, random3))\n",
    "print(\"[추가 코드] random2 == random4:\", torch.equal(random2, random4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f5ae1",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "\n",
    "- 난수 시드 (manual_seed)\n",
    "  - PyTorch는 기본적으로 실행할 때마다 다른 난수를 생성\n",
    "  - torch.manual_seed()를 쓰면 동일한 난수 시퀀스를 얻어 재현성 확보 가능\n",
    "  - 분산 학습/병렬 처리 시에는 torch.cuda.manual_seed_all() 등도 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c8dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심] 기본 dtype = torch.float32\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "\n",
    "# [핵심] dtype 지정 (정수형 int16)\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "\n",
    "# [핵심] rand + dtype=float64 → 실수 정밀도 높아짐\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)\n",
    "\n",
    "# [핵심] to() 메소드로 dtype 변환\n",
    "d = b.to(torch.int32)\n",
    "print(d)\n",
    "\n",
    "# dtype 지정 방법 종류\n",
    "\n",
    "# 1. 생성 시 dtype 지정\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "# 2. 생성 이후 변환 (메소드 체인)\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "# 3. to() 메소드 사용\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "# 4. type() 메소드 사용\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "# [핵심] 서로 다른 dtype 연산\n",
    "double_f = torch.rand(5, dtype=torch.double) #float64\n",
    "short_g = double_f.to(torch.short) # int16\n",
    "print((double_f * short_g).dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070c8f1",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- dtype 기본값\n",
    "  - PyTorch는 float형 기본값은 torch.float32\n",
    "  - torch.double = float64\n",
    "  - torch.short = int16\n",
    "\n",
    "- dtype 지정 방법\n",
    "  - 텐서 생성 시 dtype 인자로 직접 지정.\n",
    "  - .to(torch.dtype) 메소드로 변환.\n",
    "  - .double(), .float(), .short() 등 축약 메소드 사용.\n",
    "  - .type(torch.dtype) (과거 방식, 최근엔 잘 안 쓴다고 함).\n",
    "\n",
    "- 자동 형변환 (type promotion)\n",
    "  - PyTorch는 서로 다른 dtype 텐서 연산 시 더 정밀한 쪽으로 승격한다.\n",
    "  - 예: float64 ⨉ int16 → float64.\n",
    "  - 이는 정밀도 손실을 최소화하기 위한 정책."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a193c32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "[추가 코드] t1 after add_: tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "\n",
      "[추가 코드] 행렬 곱 결과:\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "[추가 코드] 요소별 곱 결과:\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "\n",
    "# [핵심] 덧셈: torch.add()와 '+' 연산자는 동일 기능\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "# [핵심] 뺄셈: torch.sub()와 '-' 연산자는 동일\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "# [핵심] 곱셈: torch.mul()과 '*' 연산자는 동일 (요소별 곱 element-wise)\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "# [핵심] 나눗셈: torch.div()와 '/' 연산자는 동일 (요소별 나눗셈 element-wise)\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)\n",
    "\n",
    "\n",
    "# [추가 코드] in-place 연산자 예시\n",
    "t1.add_(t2)  # t1 = t1 + t2 (메모리 재할당 없이 기존 텐서 변경)\n",
    "print(\"[추가 코드] t1 after add_:\", t1)\n",
    "\n",
    "print()\n",
    "\n",
    "# [추가 코드] 두 개의 2D 텐서(행렬) 생성\n",
    "A = torch.tensor([[1., 2.],\n",
    "                  [3., 4.]])  # shape (2,2)\n",
    "B = torch.tensor([[5., 6.],\n",
    "                  [7., 8.]])  # shape (2,2)\n",
    "\n",
    "# [핵심] 행렬 곱 (matrix multiplication)\n",
    "matmul1 = torch.matmul(A, B)   # 권장 함수\n",
    "matmul2 = A @ B                # Python 연산자 오버로딩\n",
    "matmul3 = A.mm(B)              # 2D 전용 (3D 이상은 지원 X)\n",
    "\n",
    "print(\"[추가 코드] 행렬 곱 결과:\")\n",
    "print(matmul1)  # >>> [[19., 22.],\n",
    "                #      [43., 50.]]\n",
    "print(matmul2)  # 동일\n",
    "print(matmul3)  # 동일\n",
    "\n",
    "# [비교] element-wise 곱\n",
    "elementwise = A * B\n",
    "print(\"[추가 코드] 요소별 곱 결과:\")\n",
    "print(elementwise)  # >>> [[ 5., 12.],\n",
    "                    #      [21., 32.]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7f21f",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- 연산자와 함수 동등성\n",
    "  - torch.add(a, b) == a + b\n",
    "  - torch.sub(a, b) == a - b\n",
    "  - torch.mul(a, b) == a * b\n",
    "  - torch.div(a, b) == a / b\n",
    "\n",
    "- element-wise 연산\n",
    "  - 위 연산들은 모두 **요소별(element-wise)**로 수행된다.\n",
    "  - 즉, 같은 위치의 값끼리 더하고 빼고 곱하고 나눈다.\n",
    "  - 결과는 같은 shape을 가짐\n",
    "\n",
    "- 행렬곱 (matmul, @, mm)\n",
    "  - 선형대수의 행렬 곱 정의에 따라 계산.\n",
    "  - (m × n) ⨉ (n × p) → (m × p) 형태로 shape이 달라진다.\n",
    "\n",
    "- in-place 연산 (add_, sub_, mul_, div_)\n",
    "  - 이름 끝에 _가 붙은 메서드는 텐서를 직접 수정한다.\n",
    "  - 메모리 절약 장점이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9716724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[ 0.0139,  0.1488],\n",
      "        [ 0.3799, -0.9602]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n",
      "[추가 코드] matmul 결과 shape: torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심] dot product (내적)\n",
    "# 1차원 벡터(동일 길이) 두 개로 스칼라(0차원 텐서) 반환\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "\n",
    "# [핵심] mm (matrix multiplication)\n",
    "# 2차원 행렬 곱 (m×n) ⨉ (n×p) = (m×p)\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())\n",
    "\n",
    "# [핵심] bmm (batch matrix multiplication)\n",
    "# 3차원 텐서 (batch, m, n) ⨉ (batch, n, p) = (batch, m, p)\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())\n",
    "\n",
    "\n",
    "# [추가 코드] 동일한 연산을 matmul(@)로도 가능 (broadcast 지원)\n",
    "t7_alt = t5 @ t6\n",
    "print(\"[추가 코드] matmul 결과 shape:\", t7_alt.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe986643",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- torch.dot\n",
    "  - 1D 벡터 내적 전용. 결과는 스칼라(0차원 텐서)\n",
    "  - 예: [2,3]·[2,1] = 2*2 + 3*1 = 7\n",
    "  - 2D 이상은 지원하지 않음 → 대신 matmul이나 @ 사용\n",
    "\n",
    "- torch.mm\n",
    "  - 2차원 행렬 곱\n",
    "  - 반드시 2차원이어야 함\n",
    "\n",
    "- torch.bmm\n",
    "  - batch 행렬 곱. 입력 shape이 반드시 3차원이어야 함\n",
    "  - (batch, m, n) × (batch, n, p) → (batch, m, p)\n",
    "  - 예: 배치 단위로 여러 개의 행렬 곱을 한 번에 수행할 때 사용 (RNN, Transformer 등에서 자주 쓰임).\n",
    "\n",
    "- torch.matmul / @\n",
    "  - 더 범용적\n",
    "  - 1D, 2D, 3D 이상까지 모두 처리 가능.\n",
    "  - batch dimension도 자동으로 브로드캐스팅."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6bacda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n",
      "\n",
      "[추가 코드] bmm result: torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심] vector × vector = dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # >>> torch.Size([]) (스칼라 결과)\n",
    "\n",
    "# [핵심] matrix × vector\n",
    "# (3,4) × (4,) → (3,)\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # >>> torch.Size([3])\n",
    "\n",
    "# [핵심] batched matrix × vector\n",
    "# (10,3,4) × (4,) → (10,3)\n",
    "# batch dimension 10개에 대해 각각 matrix×vector 연산 수행\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # >>> torch.Size([10, 3])\n",
    "\n",
    "# [핵심] batched matrix × batched matrix\n",
    "# (10,3,4) × (10,4,5) → (10,3,5)\n",
    "# batch=10, 각 배치별로 (3×4)×(4×5) 수행\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # >>> torch.Size([10, 3, 5])\n",
    "\n",
    "# [핵심] batched matrix × matrix\n",
    "# (10,3,4) × (4,5) → (10,3,5)\n",
    "# (4,5) 행렬이 브로드캐스팅되어 batch=10개 모두에 적용\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # >>> torch.Size([10, 3, 5])\n",
    "\n",
    "print()\n",
    "\n",
    "# [추가 코드] 비교: bmm은 반드시 두 입력이 3D여야 하지만, matmul은 2D/3D/브로드캐스팅 모두 지원\n",
    "t11 = torch.bmm(t7, t8)  # 같은 결과지만 bmm은 (batch, m, n)×(batch, n, p) 고정\n",
    "print(\"[추가 코드] bmm result:\", t11.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf89db",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- matmul의 범용성\n",
    "  - 1D × 1D → 스칼라(dot product)\n",
    "  - 2D × 1D → 벡터\n",
    "  - 3D × 1D → 배치 벡터\n",
    "  - 3D × 3D → 배치 행렬 곱\n",
    "  - (batch, m, n) × (n, p) => 브로드캐스팅으로 batch마다 연산\n",
    "\n",
    "- 브로드캐스팅 장점\n",
    "  - bmm는 입력이 반드시 (batch, m, n) 형태여야 함\n",
    "  - matmul은 마지막 두 차원을 행렬로 보고 나머지 차원은 자동 브로드캐스팅\n",
    "  - 따라서 고차원 배치 연산도 편리하게 작성할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심] 벡터 × 스칼라: 모든 원소에 스칼라 곱\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)  # >>> tensor([2., 4., 6.])\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 브로드캐스팅 연산\n",
    "# t3: shape (3,2), t4: shape (2,) => 마지막 dim 일치 => 브로드캐스팅 적용\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)  # 각 행에서 [4,5]를 뺀 결과\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 스칼라 연산은 add, sub, mul, div 메서드와 동일\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 정규화 함수 예시\n",
    "def normalize(x):\n",
    "    return x / 255\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)  \n",
    "print(normalize(t6).size())  # >>> torch.Size([3,28,28])\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "# [핵심] 다양한 브로드캐스팅 예시\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])   # shape (2,2)\n",
    "t8 = torch.tensor([[3, 1]])           # shape (1,2)\n",
    "t9 = torch.tensor([[5], [2]])         # shape (2,1)\n",
    "t10 = torch.tensor([7])               # shape (1,)\n",
    "\n",
    "print(t7 + t8)   # (2,2)+(1,2) → (2,2)\n",
    "print(t7 + t9)   # (2,2)+(2,1) → (2,2)\n",
    "print(t8 + t9)   # (1,2)+(2,1) → (2,2)\n",
    "print(t7 + t10)  # (2,2)+(1,)  → (2,2)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 고차원 브로드캐스팅\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)   # (3,2) → (4,3,2)로 브로드캐스트\n",
    "print(t12.shape)  # (4,3,2)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)   # (3,1) → (4,3,2)로 확장\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)   # (1,2) → (4,3,2)로 확장\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)      # (3,1,1) → (5,3,4,1)로 확장\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 브로드캐스팅 일반 규칙 확인\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # (5,3,4,1)\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # (3,1,7)\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # (3,3,3)\n",
    "\n",
    "# [주석] shape 불일치 예시 (실패)\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "# RuntimeError: size 불일치 (dim=1: 2 vs 3)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 거듭제곱 연산\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([25, 25, 25, 25])\n",
    "\n",
    "# [핵심] 각 원소별 다른 지수 적용 가능\n",
    "exp = torch.arange(1., 5.)  # [1.,2.,3.,4.]\n",
    "a = torch.arange(1., 5.)    # [1.,2.,3.,4.]\n",
    "t29 = torch.pow(a, exp)     # [1^1, 2^2, 3^3, 4^4]\n",
    "print(t29)  # >>> tensor([1., 4., 27., 256.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c00b4",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- 스칼라 연산\n",
    "  - 모든 연산(add, sub, mul, div)은 스칼라에 대해서도 자동으로 브로드캐스트 적용.\n",
    "  - 예: tensor + 2.0 ↔ tensor.add(2.0).\n",
    "\n",
    "- 브로드캐스팅 규칙\n",
    "  - 뒤에서부터 차원을 비교.\n",
    "  - 두 차원이 같거나, 둘 중 하나가 1이면 브로드캐스트 가능.\n",
    "  - 그렇지 않으면 RuntimeError.\n",
    "\n",
    "- 정규화\n",
    "  - /255 같은 간단한 스칼라 연산으로도 전체 텐서 정규화 가능.\n",
    "  - 이미지 전처리에서 매우 흔히 사용\n",
    "\n",
    "- 고차원 브로드캐스팅\n",
    "  - (batch, channel, height, width) 같은 4D 텐서에도 동일 규칙 적용.\n",
    "  - 덕분에 채널별 scaling, bias 추가, global weight 적용 등이 간단해진다.\n",
    "\n",
    "- 거듭제곱 연산\n",
    "  - torch.pow(t, 2) → 모든 원소 제곱.\n",
    "  - torch.pow(a, exp) → 원소별 지수를 다르게 적용 가능.\n",
    "  - 예: [1,2,3,4]와 [1,2,3,4]를 pow 하면 [1^1, 2^2, 3^3, 4^4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b56fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "# 단일 행 출력\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "\n",
    "# 특정 열 선택 (:는 모든 행, 1번째 열만 ) \n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "\n",
    "# 특정 원소 선택 (1행 2열의 값)\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "\n",
    "# 음수 인덱스 마지막 열\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)\n",
    "\n",
    "print()\n",
    "\n",
    "#  행 슬라이싱: 1행부터 끝까지\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
    "\n",
    "# 행 1~끝, 열 3~끝\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])\n",
    "\n",
    "print()\n",
    "\n",
    "y = torch.zeros((6, 6))\n",
    "\n",
    "# 부분 대입: 1~3행, 2번 열에 값 1 대입\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])\n",
    "\n",
    "print()\n",
    "\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "\n",
    "# 앞에서 2행 선택\n",
    "print(z[:2])\n",
    "\n",
    "# 행 1~끝, 열 1~2\n",
    "print(z[1:, 1:3])\n",
    "\n",
    "# 모든 행, 열 1~끝\n",
    "print(z[:, 1:])\n",
    "\n",
    "# 부분 영역에 값 대입\n",
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c2918",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- 기본 인덱싱\n",
    "  - x[i] → i번째 행\n",
    "  - x[:, j] → j번째 열\n",
    "  - x[i, j] → (i,j) 원소\n",
    "  - 음수 인덱스도 가능 (-1 = 마지막)\n",
    "\n",
    "- 슬라이싱\n",
    "  - 파이썬 리스트와 유사 (start:end:step)\n",
    "  - 다차원에서도 :를 조합하여 원하는 범위를 지정\n",
    "\n",
    "- 부분 대입 가능\n",
    "  - PyTorch 인덱싱은 보통 **뷰(view)**를 반환\n",
    "  - 따라서 특정 영역을 선택한 후 바로 값 대입이 가능 (y[1:4,2]=1)\n",
    "  - NumPy와 유사하게 동작 → in-place 연산으로 효율적\n",
    "\n",
    "- 형태 유지\n",
    "  - :를 사용하면 차원 유지\n",
    "  - 특정 원소만 지정하면 차원이 축소됨 (예: x[1,2] → 스칼라)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심] view / reshape\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])  # shape (2,3)\n",
    "t2 = t1.view(3, 2)         # 메모리 연속성 유지 시 shape 변경 (2,3)→(3,2)\n",
    "t3 = t1.reshape(1, 6)      # view 불가하면 내부 복사 후 (1,6) 생성\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # (8,) → (2,4)\n",
    "t5 = torch.arange(6).view(2, 3)  # (6,) → (2,3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] squeeze: 크기 1인 차원 제거\n",
    "t6 = torch.tensor([[[1], [2], [3]]])  # shape (1,3,1)\n",
    "t7 = t6.squeeze()      # 모든 1 제거 → (3,)\n",
    "t8 = t6.squeeze(0)     # 0번째 차원만 제거 → (3,1)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] unsqueeze: 특정 위치에 차원 추가\n",
    "t9 = torch.tensor([1, 2, 3])  # shape (3,)\n",
    "t10 = t9.unsqueeze(1)         # → (3,1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor([\n",
    "  [1, 2, 3],\n",
    "  [4, 5, 6]]\n",
    ")   # shape (2,3)\n",
    "t12 = t11.unsqueeze(1)            # → (2,1,3)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] flatten: 다차원 → 1차원으로 평탄화\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])  # (2,3)\n",
    "t14 = t13.flatten()                         # → (6,)\n",
    "print(t14)\n",
    "\n",
    "# flatten with start_dim\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])  # shape (2,2,2)\n",
    "t16 = torch.flatten(t15)                  # 전체 flatten → (8,)\n",
    "t17 = torch.flatten(t15, start_dim=1)     # (2,2,2) → (2,4)\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] permute: 차원 순서를 자유롭게 재배치\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)                             # (2,3,5)\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # (5,2,3)\n",
    "\n",
    "# [핵심] permute on 2D\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])  # (2,3)\n",
    "\n",
    "t20 = torch.permute(t19, (0, 1))            # (2,3) 그대로\n",
    "t21 = torch.permute(t19, (1, 0))            # (3,2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# [핵심] transpose: 두 차원만 교환\n",
    "t22 = torch.transpose(t19, 0, 1)  # (2,3) → (3,2)\n",
    "print(t22)\n",
    "\n",
    "# [핵심] t(): 2D 텐서 전용 전치 (transpose(0,1)와 동일)\n",
    "t23 = torch.t(t19)                # (2,3) → (3,2)\n",
    "print(t23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32e99e",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- view vs reshape\n",
    "  - view: 메모리 연속성 필요 → 더 빠름\n",
    "  - reshape: 내부적으로 view 시도, 안 되면 복사 발생 → 더 안전\n",
    "\n",
    "- squeeze / unsqueeze\n",
    "  - squeeze: 크기 1인 차원 제거 → 데이터는 그대로, 차원만 바뀜\n",
    "  - unsqueeze: 특정 위치에 크기 1인 차원 추가\n",
    "\n",
    "- flatten\n",
    "  - 모든 차원을 평탄화하거나 특정 차원부터 평탄화 가능\n",
    "  - Fully Connected Layer 입력 준비 과정에서 주로 사용됨\n",
    "\n",
    "- permute vs transpose vs t\n",
    "  - permute: 차원 순서를 임의로 변경 (고차원 가능).\n",
    "  - transpose: 두 차원만 교환.\n",
    "  - t(): 2D 전용 전치.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c36bf5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# [핵심] 서로 다른 shape (dim=1만 다름) 텐서 결합\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)  # 두 번째 차원(dim=1)을 따라 합침\n",
    "print(t4.shape)  # (2, 1+3+2, 3) = (2,6,3)\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 1차원 텐서 결합\n",
    "t5 = torch.arange(0, 3)   # [0,1,2]\n",
    "t6 = torch.arange(3, 8)   # [3,4,5,6,7]\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # torch.Size([8])\n",
    "print(t7)        # [0,1,2,3,4,5,6,7]\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 2차원 텐서 결합\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)   # [[0,1,2],[3,4,5]]\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # [[6,7,8],[9,10,11]]\n",
    "\n",
    "# dim=0: 행(row) 방향 연결 → 세로 쌓기\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # (4,3)\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "# dim=1: 열(column) 방향 연결 → 가로 확장\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # (2,6)\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 여러 텐서 한꺼번에 cat 가능\n",
    "t12 = torch.arange(0, 6).reshape(2, 3) # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3) # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3) # torch.Size([2, 3])\n",
    "\n",
    "# 세로 쌓기\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # (6,3)\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "# 가로 확장\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # (2,9)\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 3차원 텐서 결합\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)   # shape (1,2,3)\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # shape (1,2,3)\n",
    "\n",
    "# dim=0: batch 크기 증가\n",
    "t19 = torch.cat((t17, t18), dim=0)  # (2,2,3)\n",
    "print(t19.size())\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "# dim=1: 중간(행) 축 확장\n",
    "t20 = torch.cat((t17, t18), dim=1)  # (1,4,3)\n",
    "print(t20.size())\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "# dim=2: 마지막(열) 축 확장\n",
    "t21 = torch.cat((t17, t18), dim=2)  # (1,2,6)\n",
    "print(t21.size())\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e87d49",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- 기본 규칙\n",
    "  - torch.cat(tensors, dim)은 dim 차원을 따라 연결한다.\n",
    "  - 연결하려는 텐서들은 dim을 제외한 다른 차원이 모두 동일해야 한다.\n",
    "\n",
    "- 1D 텐서 결합\n",
    "  - 단순히 리스트 이어붙이기처럼 동작.\n",
    "\n",
    "- 2D 텐서 결합\n",
    "  - dim=0: 행을 늘려 세로 쌓기 → 데이터 샘플(batch) 수 확장.\n",
    "  - dim=1: 열을 늘려 가로 확장 → 특성(feature) 수 확장.\n",
    "\n",
    "- 3D 이상 텐서 결합\n",
    "  - dim=0: batch dimension 확장.\n",
    "  - dim=1: sequence length, height 등 중간 차원 확장.\n",
    "  - dim=2: 채널 수, feature dimension 확장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb5ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# [핵심] stack: 새로운 차원(dim)을 추가하여 합침\n",
    "t3 = torch.stack([t1, t2], dim=0)  \n",
    "# shape (2,2,3) : [2개 텐서, 각 (2,3)]\n",
    "# cat으로 동일하게 만들려면 각 텐서에 dim=0 차원 추가 후 cat\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))  # torch.Size([2, 2, 3]) True\n",
    "\n",
    "# dim=1로 stack → shape (2,2,3)에서 두 번째 차원에 추가\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "# dim=2로 stack → 마지막 차원에 추가\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "print()\n",
    "\n",
    "# [핵심] 1차원 텐서 예제\n",
    "t9 = torch.arange(0, 3)   # [0,1,2] (3,)\n",
    "t10 = torch.arange(3, 6)  # [3,4,5] (3,)\n",
    "\n",
    "print(t9.size(), t10.size())  # (3,), (3,)\n",
    "\n",
    "# stack: dim=0 -> (2,3)\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # (2,3)\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "# cat: 두 텐서를 (1,3)으로 차원 확장 후 concat -> 동일 결과\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))  # True\n",
    "\n",
    "# stack: dim=1 → (3,2)\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # (3,2)\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "\n",
    "# cat: 두 텐서를 (3,1)으로 확장 후 concat -> 동일 결과\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))  # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae6d20",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- stack\n",
    "  - 새로운 차원을 추가하여 텐서를 쌓는다\n",
    "  - 입력 텐서들의 shape이 정확히 같아야 함\n",
    "  - 결과 shape = (num_tensors, ...) 또는 지정된 위치에 새로운 차원 추가\n",
    "  - 예: [3] + [3] → (2,3) (dim=0), (3,2) (dim=1).\n",
    "\n",
    "- cat\n",
    "  - 기존 차원 중 하나를 따라 이어붙임\n",
    "  - 결합 전, 원하는 위치에 unsqueeze로 차원을 추가하면 stack과 같은 결과를 낼 수 있음\n",
    "  - 따라서 stack은 사실상 unsqueeze + cat\n",
    "\n",
    "- 차이점 요약\n",
    "  - stack: 차원 추가가 기본 동작\n",
    "  - cat: 차원 유지가 기본, 단순 이어붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "788290c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# [핵심] vstack: 수직(vertical) 방향으로 쌓기\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "# 2D 텐서 예제\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "print(t6)\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "# 3D 텐서 예제\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "]) \n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "]) \n",
    "print(t8.shape)\n",
    " # >>> (2, 2, 3)\n",
    "\n",
    "# vstack은 dim=0 방향으로 cat\n",
    "t9 = torch.vstack([t7, t8])  # shape (4,2,3)\n",
    "print(t9.shape) # >>> (4, 2, 3)\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print()\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# [핵심] hstack: 수평(horizontal) 방향으로 이어붙이기\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)  # tensor([1,2,3,4,5,6])\n",
    "\n",
    "# 2D 텐서 예제\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# tensor([[1,4],\n",
    "#         [2,5],\n",
    "#         [3,6]])\n",
    "\n",
    "# 3D 텐서 예제\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "]) \n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "# hstack은 2D 이상일 때 dim=1 방향 cat\n",
    "t18 = torch.hstack([t16, t17])  # shape (2,4,3)\n",
    "print(t18.shape)\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46b7ff",
   "metadata": {},
   "source": [
    "##### 고찰 및 기술적 사항\n",
    "- vstack\n",
    "  - 수직 쌓기 = cat(..., dim=0)과 동일.\n",
    "  - 1D 텐서는 자동으로 (N,) → (1,N) 변환 후 쌓임.\n",
    "  - 즉, 항상 \"위아래로 행(row)\" 추가되는 느낌.\n",
    "  - 예: 두 벡터 [1,2,3]과 [4,5,6] → (2,3) 행렬.\n",
    "\n",
    "- hstack\n",
    "  - 수평 쌓기 = cat(..., dim=1) (2D 이상).\n",
    "  - 1D 텐서는 단순히 이어붙이기(cat(..., dim=0))와 동일.\n",
    "  - 즉, \"옆으로 열(column)\" 추가되는 느낌.\n",
    "\n",
    "- 차이 요약\n",
    "  - vstack: 행 방향 확장 (dim=0).\n",
    "  - hstack: 열 방향 확장 (dim=1 for 2D 이상).\n",
    "  - 결국 cat의 단순한 래퍼 → NumPy 스타일로 직관성↑."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14535d89",
   "metadata": {},
   "source": [
    "#### 숙제 1 후기\n",
    "\n",
    "이번 과제를 통해서 PyTorch의 텐서 연산 기초를 처음부터 끝까지 실습하면서, 단순히 코드를 실행하는 수준을 넘어 각 연산의 의미와 실무적 활용까지 생각해볼 수 있었다.\n",
    "\n",
    "1. 텐서 생성과 속성\n",
    "\n",
    "torch.Tensor, torch.tensor, torch.as_tensor의 차이를 직접 확인하면서 데이터 복사 여부와 원본 데이터 반영 차이가 명확히 이해되었다.\n",
    "\n",
    "CPU ↔ GPU 이동, dtype, requires_grad 같은 속성을 확인하면서 “텐서 하나가 단순 숫자 모음이 아니라, 계산 그래프 위의 데이터 컨테이너”라는 걸 느낄 수 있었다.\n",
    "\n",
    "2. 텐서의 차원과 구조\n",
    "\n",
    "0D(스칼라)부터 5D까지 텐서를 직접 만들어보니, shape과 ndim(rank) 개념이 확실히 잡혔다.\n",
    "\n",
    "squeeze와 unsqueeze로 차원을 제거·추가하는 과정에서, CNN/RNN 같은 모델에서 입력 차원 맞추기가 왜 중요한지 실감했다.\n",
    "\n",
    "flatten으로 FC layer 입력을 준비하는 과정은 실제 딥러닝 학습 코드와 바로 연결된다는 점이 흥미로웠다.\n",
    "\n",
    "3. 산술 연산과 브로드캐스팅\n",
    "\n",
    "+, -, *, / 같은 연산자와 torch.add, torch.mul 같은 함수가 사실상 동일하다는 점이 직관적이었다.\n",
    "\n",
    "element-wise 연산 vs 행렬 곱(matmul) 차이를 분명히 알게 되었고, 특히 bmm, matmul에서 배치 단위 행렬 곱이 자동으로 브로드캐스팅되는 걸 확인하면서 “수학적 정의와 실제 구현이 연결되는 방식”을 알게 되었다.\n",
    "\n",
    "브로드캐스팅 규칙을 실습하면서 차원 불일치 오류를 막는 방법도 익힐 수 있었다.\n",
    "\n",
    "4. 차원 조작\n",
    "\n",
    "view와 reshape의 차이(메모리 연속성 vs 안전한 변환), permute와 transpose의 차이(임의 차원 순서 변경 vs 두 축 교환) 등을 실습해보니, 앞으로 shape mismatch 오류가 날 때 당황하지 않고 해결할 수 있을 것 같다.\n",
    "\n",
    "expand vs repeat 차이를 보면서, 메모리 효율성과 안전성의 트레이드오프도 이해했다.\n",
    "\n",
    "5. 텐서 결합\n",
    "\n",
    "torch.cat, torch.stack, torch.vstack, torch.hstack을 비교하면서, **차원 확장(stack)**과 **기존 차원 병합(cat)**의 차이를 정리할 수 있었다.\n",
    "\n",
    "vstack, hstack은 결국 cat의 래퍼이지만, NumPy 스타일처럼 직관적이어서 데이터 전처리에 더 자주 쓸 수 있을 것 같다.\n",
    "\n",
    "6. 종합적인 배움\n",
    "\n",
    "이번 과제는 단순히 API를 나열하는 게 아니라, 각 함수와 메서드가 딥러닝 모델 구조 설계에서 어떤 역할을 하는지 직접 연결해서 이해할 수 있게 해줬다.\n",
    "\n",
    "특히 shape 변환과 브로드캐스팅은 “머리로는 알지만 손으로 안 해보면 헷갈리는 부분”인데, 이번 실습을 통해 확실히 감을 잡았다.\n",
    "\n",
    "7. 느낀 점과 하고 싶은 말\n",
    "\n",
    "처음에는 단순한 코드 실행 과제라고 생각했는데, 각 블럭마다 주석과 고찰을 달다 보니 실제 딥러닝 코드에서 자주 만나는 이슈들과 자연스럽게 연결되었다.\n",
    "\n",
    "“왜 이 연산이 필요한가?”를 고민하게 되었고, 앞으로 모델 구현 시 shape 관련 버그를 더 빨리 해결할 자신감이 생겼다.\n",
    "\n",
    "PyTorch는 NumPy와 비슷하면서도, 자동 미분과 GPU 지원 때문에 더 엄격하게 shape와 dtype을 다뤄야 한다는 것도 배웠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e8bc3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "link_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
